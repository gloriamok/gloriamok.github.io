---
layout: single
title:  "딥러닝을 위한 파이토치 입문 - 텐서"
---

## 딥러닝을 위한 파이토치 입문 - 텐서
<a href="https://youtu.be/6SF_qAd99Yg">유튜브 링크: 딥러닝을 위한 파이토치 입문 - 텐서</a>

#### Pytorch vs. Tensorflow

- 둘 다 GPU와 함께 딥러닝을 이용할 수 있게 해주는 프레임워크

- Pytorch는 내부적인 CUDA API를 통해 GPU연산을 제공

- 최근 논문에는 Pytorch가 더 많이 사용되고 있음

- 심지어 Tensorflow를 만든 구글 논문에서도 Pytorch를 쓰고 있음

- Pytorch는 직관적으로 이해하기 편함

### 1강. Tensor
- Pytorch의 기본 단위/라이브러리
- torch tensor는 numpy array와 사용법이 유사함


```python
# Torch: The torch package contains data structures for multi-dimensional tensors
#        and matehmatical operations over these are defined.
```


```python
import torch
import numpy as np
```

empty(size) 함수로 빈 텐서를 생성


```python
x = torch.empty(5,4)
print(x)
```

    tensor([[9.2755e-39, 1.0561e-38, 7.0715e-39, 6.8878e-39],
            [8.9082e-39, 8.9082e-39, 1.0194e-38, 9.1837e-39],
            [4.6837e-39, 9.9184e-39, 9.0000e-39, 1.0561e-38],
            [1.0653e-38, 4.1327e-39, 8.9082e-39, 9.8265e-39],
            [9.4592e-39, 1.0561e-38, 1.0286e-38, 1.0102e-38]])
    


```python
# 1 행렬
torch.ones(3,3)
```




    tensor([[1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.]])




```python
# 0 행렬
torch.zeros(2)
```




    tensor([0., 0.])




```python
# random 함수
torch.rand(5,6)
```




    tensor([[0.0177, 0.1933, 0.7950, 0.4988, 0.5507, 0.6176],
            [0.6692, 0.7715, 0.5021, 0.5853, 0.1138, 0.2583],
            [0.9772, 0.1763, 0.1570, 0.3077, 0.2413, 0.8652],
            [0.3559, 0.5762, 0.7934, 0.2505, 0.5976, 0.4725],
            [0.5011, 0.9839, 0.7965, 0.7170, 0.8160, 0.0257]])



list나 array가 주어져 있을 경우 tensor로 변환


```python
l = [13,4]
r = np.array([4,56,7])

torch.tensor(l)
```




    tensor([13,  4])




```python
torch.tensor(r)
```




    tensor([ 4, 56,  7], dtype=torch.int32)



size 함수로 사이즈 확인


```python
torch.tensor(r).size()
```




    torch.Size([3])




```python
x.size()
```




    torch.Size([5, 4])



type 함수로 타입 확인


```python
type(x)
```




    torch.Tensor




```python
x = torch.rand(2,2)
y = torch.rand(2,2)
print(x)
print(y)
```

    tensor([[0.1602, 0.3786],
            [0.7703, 0.8538]])
    tensor([[0.6397, 0.1728],
            [0.8106, 0.2421]])
    

텐서 간의 연산 3가지 방법


```python
print(x+y)
print(torch.add(x,y))
print(y.add(x))
```

    tensor([[0.7999, 0.5514],
            [1.5809, 1.0959]])
    tensor([[0.7999, 0.5514],
            [1.5809, 1.0959]])
    tensor([[0.7999, 0.5514],
            [1.5809, 1.0959]])
    

연산 후 변수 대체(inplace) 한 번에 하는 법


```python
# y에 x를 더해서 y에 넣는다 (y += x)
y.add_(x)
```




    tensor([[0.7999, 0.5514],
            [1.5809, 1.0959]])




```python
y
```




    tensor([[0.7999, 0.5514],
            [1.5809, 1.0959]])



인덱싱 하는 법 (indexing, subarray)


```python
# 1행 1열 인덱싱
y[1,1]
```




    tensor(1.0959)




```python
# 1열 인덱싱
y[:,1]
```




    tensor([0.5514, 1.0959])



#### view 함수
- tensor의 사이즈를 자유롭게 바꿔줌
- CNN인 경우에 Convolution layer를 연산하다 마지막에 정사각형 이미지를 일렬로 펴줄 때 사용


```python
x = torch.rand(8,8)
print(x)
print()

# 텐서를 일렬로 변환
# 8X8=64=4X16
# 열 기준으로 나열
print(x.view(64))
print()

print(x.view(4,16))
print()

# 한 숫자는 아는데 곱해지는 다른 숫자 모를 때 -1을 넣음.
# -1에 4가 자동으로 들어감
print(x.view(-1,16)) # x.view(4,16) 와 같은 결과

# 나머지 값이 고정된 상태여야 -1 사용 가능
# print(x.view(-1,16,-1))는 미지수가 2개 있기 때문에 불가
print(x.view(-1,4,4)) #는 가능
```

    tensor([[0.9609, 0.7584, 0.8624, 0.0621, 0.0635, 0.3722, 0.5378, 0.1184],
            [0.4923, 0.0734, 0.6578, 0.7427, 0.1559, 0.9920, 0.1309, 0.1635],
            [0.5702, 0.8633, 0.4600, 0.4492, 0.7119, 0.7203, 0.5062, 0.0985],
            [0.1185, 0.9494, 0.0526, 0.0354, 0.7567, 0.7949, 0.7567, 0.7099],
            [0.6943, 0.8203, 0.3028, 0.0806, 0.8755, 0.1984, 0.9588, 0.3842],
            [0.6262, 0.1309, 0.5831, 0.6257, 0.1641, 0.8256, 0.4824, 0.8090],
            [0.0082, 0.5660, 0.2309, 0.2505, 0.7738, 0.7985, 0.8798, 0.3106],
            [0.3669, 0.0245, 0.9864, 0.6415, 0.9527, 0.7203, 0.2599, 0.4918]])
    
    tensor([0.9609, 0.7584, 0.8624, 0.0621, 0.0635, 0.3722, 0.5378, 0.1184, 0.4923,
            0.0734, 0.6578, 0.7427, 0.1559, 0.9920, 0.1309, 0.1635, 0.5702, 0.8633,
            0.4600, 0.4492, 0.7119, 0.7203, 0.5062, 0.0985, 0.1185, 0.9494, 0.0526,
            0.0354, 0.7567, 0.7949, 0.7567, 0.7099, 0.6943, 0.8203, 0.3028, 0.0806,
            0.8755, 0.1984, 0.9588, 0.3842, 0.6262, 0.1309, 0.5831, 0.6257, 0.1641,
            0.8256, 0.4824, 0.8090, 0.0082, 0.5660, 0.2309, 0.2505, 0.7738, 0.7985,
            0.8798, 0.3106, 0.3669, 0.0245, 0.9864, 0.6415, 0.9527, 0.7203, 0.2599,
            0.4918])
    
    tensor([[0.9609, 0.7584, 0.8624, 0.0621, 0.0635, 0.3722, 0.5378, 0.1184, 0.4923,
             0.0734, 0.6578, 0.7427, 0.1559, 0.9920, 0.1309, 0.1635],
            [0.5702, 0.8633, 0.4600, 0.4492, 0.7119, 0.7203, 0.5062, 0.0985, 0.1185,
             0.9494, 0.0526, 0.0354, 0.7567, 0.7949, 0.7567, 0.7099],
            [0.6943, 0.8203, 0.3028, 0.0806, 0.8755, 0.1984, 0.9588, 0.3842, 0.6262,
             0.1309, 0.5831, 0.6257, 0.1641, 0.8256, 0.4824, 0.8090],
            [0.0082, 0.5660, 0.2309, 0.2505, 0.7738, 0.7985, 0.8798, 0.3106, 0.3669,
             0.0245, 0.9864, 0.6415, 0.9527, 0.7203, 0.2599, 0.4918]])
    
    tensor([[0.9609, 0.7584, 0.8624, 0.0621, 0.0635, 0.3722, 0.5378, 0.1184, 0.4923,
             0.0734, 0.6578, 0.7427, 0.1559, 0.9920, 0.1309, 0.1635],
            [0.5702, 0.8633, 0.4600, 0.4492, 0.7119, 0.7203, 0.5062, 0.0985, 0.1185,
             0.9494, 0.0526, 0.0354, 0.7567, 0.7949, 0.7567, 0.7099],
            [0.6943, 0.8203, 0.3028, 0.0806, 0.8755, 0.1984, 0.9588, 0.3842, 0.6262,
             0.1309, 0.5831, 0.6257, 0.1641, 0.8256, 0.4824, 0.8090],
            [0.0082, 0.5660, 0.2309, 0.2505, 0.7738, 0.7985, 0.8798, 0.3106, 0.3669,
             0.0245, 0.9864, 0.6415, 0.9527, 0.7203, 0.2599, 0.4918]])
    tensor([[[0.9609, 0.7584, 0.8624, 0.0621],
             [0.0635, 0.3722, 0.5378, 0.1184],
             [0.4923, 0.0734, 0.6578, 0.7427],
             [0.1559, 0.9920, 0.1309, 0.1635]],
    
            [[0.5702, 0.8633, 0.4600, 0.4492],
             [0.7119, 0.7203, 0.5062, 0.0985],
             [0.1185, 0.9494, 0.0526, 0.0354],
             [0.7567, 0.7949, 0.7567, 0.7099]],
    
            [[0.6943, 0.8203, 0.3028, 0.0806],
             [0.8755, 0.1984, 0.9588, 0.3842],
             [0.6262, 0.1309, 0.5831, 0.6257],
             [0.1641, 0.8256, 0.4824, 0.8090]],
    
            [[0.0082, 0.5660, 0.2309, 0.2505],
             [0.7738, 0.7985, 0.8798, 0.3106],
             [0.3669, 0.0245, 0.9864, 0.6415],
             [0.9527, 0.7203, 0.2599, 0.4918]]])
    

tensor에서 numpy array로 변환


```python
y = x.numpy()
print(y)
```

    [[0.9608807  0.75838095 0.8623952  0.06210738 0.06347865 0.37219214
      0.5377801  0.11839724]
     [0.49230343 0.0733909  0.6577639  0.7426685  0.1558876  0.9920472
      0.13090938 0.16346544]
     [0.57016426 0.8632922  0.45997983 0.4491688  0.7118903  0.72026604
      0.5062248  0.09845442]
     [0.11849904 0.9494247  0.05261171 0.03537315 0.75669414 0.79493624
      0.7566905  0.7099154 ]
     [0.69432265 0.8203324  0.30281723 0.08055097 0.8755239  0.19844323
      0.95882565 0.3842287 ]
     [0.626184   0.13092607 0.5831461  0.62569135 0.16408205 0.8256468
      0.48239493 0.80897814]
     [0.00817329 0.5660144  0.23092961 0.25053442 0.7737721  0.7984659
      0.87981063 0.31055498]
     [0.36689413 0.02451593 0.98636687 0.6415478  0.9527004  0.7203196
      0.2599097  0.49177027]]
    


```python
type(y)
```




    numpy.ndarray



#### item 함수
- tensor가 scalar일 때(원소가 1개일 때)만 사용 가능 
- tensor 안에 값을 숫자 형태로 불러오고 싶을 때
- loss 값을 뽑아내고 싶을 때나 값을 사용할 때 (ex. plotting할 때)



```python
x = torch.ones(1)
x
```




    tensor([1.])




```python
x.item()
```




    1.0



파이토치 관련 추천 유튜브 강의

1. <a href="https://www.youtube.com/watch?v=6SF_qAd99Yg&list=PLHOsBEAyYj3xf4i20sCA5o8MgVW5sIiHD&index=9">텐서 개념</a>
2. <a href="https://www.youtube.com/watch?v=_4gEHlsvSZ8&list=PLHOsBEAyYj3xf4i20sCA5o8MgVW5sIiHD&index=10">역전파</a> 
3. <a href="https://www.youtube.com/watch?v=8PnxJ3s3Cwo&list=PLHOsBEAyYj3xf4i20sCA5o8MgVW5sIiHD&index=11">데이터 불러오기 + 커스터마이징</a>
4. <a href="https://www.youtube.com/watch?v=Gm9Spk2Nmj0&list=PLHOsBEAyYj3xf4i20sCA5o8MgVW5sIiHD&index=12">기본모델 구축 및 저장</a>
5. <a href="https://www.youtube.com/watch?v=eYjcN_zwUP4&list=PLHOsBEAyYj3xf4i20sCA5o8MgVW5sIiHD&index=13">스케줄링</a>
6. <a href="https://www.youtube.com/watch?v=8WCTCBHBnP0&list=PLHOsBEAyYj3xf4i20sCA5o8MgVW5sIiHD&index=14">비지도 학습 구현</a>
